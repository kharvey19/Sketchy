{
  "config": {
    "name": "both",
    "attention_type": "both",
    "description": "LoRA applied to both self and cross-attention layers"
  },
  "modified_layers": [
    "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q",
    "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k",
    "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v",
    "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0",
    "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q",
    "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k",
    "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v",
    "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0",
    "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q",
    "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k",
    "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v",
    "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0",
    "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q",
    "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k",
    "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v",
    "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0",
    "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q",
    "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k",
    "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v",
    "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0",
    "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q",
    "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k",
    "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v",
    "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q",
    "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k",
    "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v",
    "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q",
    "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k",
    "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v",
    "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q",
    "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k",
    "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v",
    "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q",
    "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k",
    "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v",
    "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q",
    "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k",
    "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v",
    "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q",
    "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k",
    "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v",
    "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q",
    "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k",
    "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v",
    "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q",
    "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k",
    "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v",
    "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0",
    "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q",
    "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k",
    "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v",
    "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0",
    "mid_block.attentions.0.transformer_blocks.0.attn2.to_q",
    "mid_block.attentions.0.transformer_blocks.0.attn2.to_k",
    "mid_block.attentions.0.transformer_blocks.0.attn2.to_v",
    "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0"
  ],
  "timestamp": "20241210_215244"
}