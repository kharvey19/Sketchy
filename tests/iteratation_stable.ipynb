{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from lpips import LPIPS  # Requires installation of lpips\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CLIP model for similarity scoring\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load LPIPS model for perceptual similarity\n",
    "lpips_model = LPIPS(net=\"alex\").to(\"cuda\")\n",
    "\n",
    "def truncate_prompt(prompt, max_length=77):\n",
    "    \"\"\"\n",
    "    Truncate the prompt to fit within the token limit for the CLIP model, considering tokenized length.\n",
    "    \"\"\"\n",
    "    from transformers import CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    tokens = tokenizer(prompt, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    # Decode back the truncated prompt\n",
    "    truncated_prompt = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    print(f\"Prompt truncated to: {truncated_prompt}\")\n",
    "    return truncated_prompt\n",
    "\n",
    "\n",
    "# Function to calculate SSIM\n",
    "def calculate_ssim(image1, image2):\n",
    "    image1 = np.array(image1.convert(\"L\"))\n",
    "    image2 = np.array(image2.convert(\"L\"))\n",
    "    ssim, _ = compare_ssim(image1, image2, full=True)\n",
    "    return ssim\n",
    "\n",
    "# Function to calculate PSNR\n",
    "def calculate_psnr(image1, image2):\n",
    "    image1 = np.array(image1)\n",
    "    image2 = np.array(image2)\n",
    "    return compare_psnr(image1, image2, data_range=255)\n",
    "\n",
    "# Function to calculate CLIP similarity\n",
    "def calculate_clip_score(image, text):\n",
    "    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    outputs = clip_model(**inputs)\n",
    "    return outputs.logits_per_image.item()\n",
    "\n",
    "def setup_stable_diffusion(seed=42):\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(device)\n",
    "    pipe.safety_checker = None\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def process_enhanced_prompt(input_image_path, prompt, num_iterations=5):\n",
    "    pipeline = setup_stable_diffusion()\n",
    "\n",
    "    # Define the enhanced prompt\n",
    "    prompt = (\n",
    "        \"highly detailed police sketch, black and white pencil drawing, \"\n",
    "        \"professional forensic artist style, detailed shading, portrait of \" + prompt\n",
    "    )\n",
    "\n",
    "    # Load the input image\n",
    "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "    # Lists to store metrics\n",
    "    ssim_scores, psnr_scores, lpips_scores, clip_scores = [], [], [], []\n",
    "\n",
    "    # Use a consistent random seed for reproducibility\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "    # Latents initialization\n",
    "    latents = None\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Iteration {i + 1}: Generating image with prompt: {prompt}\")\n",
    "\n",
    "        prompt = truncate_prompt(prompt, max_length=77)\n",
    "\n",
    "        # Generate latents for the first iteration if not already defined\n",
    "        if latents is None:\n",
    "            height, width = pipeline.unet.config.sample_size * 8, pipeline.unet.config.sample_size * 8\n",
    "            latents = torch.randn(\n",
    "                (1, pipeline.unet.in_channels, height // 8, width // 8),\n",
    "                generator=generator,\n",
    "                device=pipeline.device,\n",
    "                dtype=torch.float16,\n",
    "            )\n",
    "\n",
    "        # Generate an image using the latents\n",
    "        output = pipeline(prompt, latents=latents, return_dict=True, guidance_scale=7.5)\n",
    "        generated_image = output.images[0]\n",
    "\n",
    "        # Resize the input image to match the generated image dimensions\n",
    "        resized_input_image = input_image.resize(generated_image.size)\n",
    "\n",
    "        # Display the generated image\n",
    "        display(generated_image)\n",
    "\n",
    "        # Calculate metrics\n",
    "        ssim = calculate_ssim(resized_input_image, generated_image)\n",
    "        ssim_scores.append(ssim)\n",
    "        print(f\"SSIM with input image at iteration {i + 1}: {ssim}\")\n",
    "\n",
    "        psnr = calculate_psnr(resized_input_image, generated_image)\n",
    "        psnr_scores.append(psnr)\n",
    "        print(f\"PSNR with input image at iteration {i + 1}: {psnr}\")\n",
    "\n",
    "        lpips_value = lpips_model(\n",
    "            torch.tensor(np.array(resized_input_image)).permute(2, 0, 1).unsqueeze(0).float().to(\"cuda\"),\n",
    "            torch.tensor(np.array(generated_image)).permute(2, 0, 1).unsqueeze(0).float().to(\"cuda\")\n",
    "        ).item()\n",
    "        lpips_scores.append(lpips_value)\n",
    "        print(f\"LPIPS with input image at iteration {i + 1}: {lpips_value}\")\n",
    "\n",
    "        clip_score = calculate_clip_score(generated_image, prompt)\n",
    "        clip_scores.append(clip_score)\n",
    "        print(f\"CLIP score for iteration {i + 1}: {clip_score}\")\n",
    "\n",
    "        # Update prompt if necessary\n",
    "        if i == 0:\n",
    "            prompt += \", add more detail to the hairstyle\"\n",
    "        elif i == 1:\n",
    "            prompt += \", emphasize the eyes and eyebrows\"\n",
    "        elif i == 2:\n",
    "            prompt += \", add sharper jawline definition\"\n",
    "\n",
    "    return ssim_scores, psnr_scores, lpips_scores, clip_scores\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_image_path = \"/content/00002.jpg\"  # Replace with the path to your input image\n",
    "prompt = \"The person is described as male around 30-35 years old with an oval face, defined cheekbones, medium-length straight hair, and light skin tone.\"\n",
    "\n",
    "# Process the enhanced prompt and calculate metrics\n",
    "ssim_scores, psnr_scores, lpips_scores, clip_scores = process_enhanced_prompt(input_image_path, prompt)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFinal Metrics Across Iterations:\")\n",
    "print(f\"SSIM Scores: {ssim_scores}\")\n",
    "print(f\"PSNR Scores: {psnr_scores}\")\n",
    "print(f\"LPIPS Scores: {lpips_scores}\")\n",
    "print(f\"CLIP Scores: {clip_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Explanation:\n",
    "# 1. Structural Similarity Index (SSIM):\n",
    "#    - Measures structural similarity (luminance, contrast, and structure) between two images.\n",
    "#    - Range: 0 to 1, where 1 indicates identical structure.\n",
    "#    - Higher SSIM indicates better structural alignment between iterations.\n",
    "\n",
    "# 2. Peak Signal-to-Noise Ratio (PSNR):\n",
    "#    - Measures pixel-level distortion or noise between two images.\n",
    "#    - Higher PSNR (typically 20-50) indicates lower distortion and clearer images.\n",
    "\n",
    "# 3. Cosine Similarity:\n",
    "#    - Compares the semantic similarity of image embeddings (high-level features).\n",
    "#    - Range: -1 to 1, where 1 means identical embeddings and -1 means complete dissimilarity.\n",
    "#    - Higher values indicate better semantic consistency between iterations.\n",
    "\n",
    "# 4. CLIP Score:\n",
    "#    - Measures the similarity between a generated image and the input text prompt using the CLIP model.\n",
    "#    - Higher scores indicate that the generated image aligns better with the input prompt.\n",
    "\n",
    "# 5. Learned Perceptual Image Patch Similarity (LPIPS):\n",
    "#    - Evaluates perceptual similarity between two images based on learned features.\n",
    "#    - Range: 0 to 1, where lower scores indicate higher perceptual similarity.\n",
    "#    - Useful for assessing how close two images are in terms of human perception.\n",
    "\n",
    "# 6. Fr√©chet Inception Distance (FID):\n",
    "#    - Measures the quality and diversity of generated images compared to reference images.\n",
    "#    - Lower FID scores indicate that the generated images are closer to the reference distribution.\n",
    "#    - Typically used to evaluate overall image quality in generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust iterations to align with metrics (start from 2nd iteration)\n",
    "iterations = list(range(1, len(ssim_scores) + 1))\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# SSIM Plot\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(iterations, ssim_scores, label=\"SSIM\", marker='o', color='blue')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"SSIM\")\n",
    "plt.title(\"SSIM Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# PSNR Plot\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(iterations, psnr_scores, label=\"PSNR\", marker='o', color='green')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.title(\"PSNR Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# # Cosine Similarity Plot\n",
    "# plt.subplot(1, 5, 3)\n",
    "# plt.plot(iterations, cosine_similarities, label=\"Cosine Similarity\", marker='o', color='red')\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Cosine Similarity\")\n",
    "# plt.title(\"Cosine Similarity Over Iterations\")\n",
    "# plt.grid(True)\n",
    "\n",
    "# CLIP Score Plot\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(iterations, clip_scores, label=\"CLIP Score\", marker='o', color='purple')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"CLIP Score\")\n",
    "plt.title(\"CLIP Score Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# LPIPS Plot\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(iterations, lpips_scores, label=\"LPIPS\", marker='o', color='orange')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"LPIPS\")\n",
    "plt.title(\"LPIPS Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
