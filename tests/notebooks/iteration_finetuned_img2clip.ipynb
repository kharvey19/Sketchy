{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "import torch.nn as nn\n",
    "from lpips import LPIPS\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "projection_layer = nn.Linear(512, 768).to(\"cuda\")\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear, r, lora_alpha, lora_dropout):\n",
    "        super().__init__()\n",
    "        self.original_linear = original_linear\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(original_linear.in_features, r))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(r, original_linear.out_features))\n",
    "        self.scaling = lora_alpha / r\n",
    "\n",
    "        # Initialize parameters\n",
    "        nn.init.normal_(self.lora_A, std=0.02)\n",
    "        nn.init.normal_(self.lora_B, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.original_linear(x)\n",
    "        if self.r > 0:\n",
    "            lora_result = (self.lora_dropout(x) @ self.lora_A @ self.lora_B) * self.scaling\n",
    "            return result + lora_result\n",
    "        return result\n",
    "\n",
    "\n",
    "def apply_lora_to_model(model, r=16, lora_alpha=32, lora_dropout=0.1):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(substr in name for substr in ['q_proj', 'v_proj', 'k_proj']):\n",
    "            parent = model\n",
    "            for name_part in name.split('.')[:-1]:\n",
    "                parent = getattr(parent, name_part)\n",
    "            layer_name = name.split('.')[-1]\n",
    "            setattr(parent, layer_name, LoRALinear(module, r, lora_alpha, lora_dropout))\n",
    "\n",
    "def setup_clip_model(checkpoint_path, device):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    apply_lora_to_model(model)  # Apply LoRA modifications\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "    model = model.to(device).eval()\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def generate_image_with_sketch_and_embeddings(\n",
    "    input_image_path, prompt, clip_model, clip_processor, stable_diffusion, strength=0.3, guidance_scale=7.5\n",
    "):\n",
    "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "    # Preprocess the image and prompt for CLIP\n",
    "    inputs = clip_processor(text=[prompt], images=input_image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Generate CLIP embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        text_features = clip_model.get_text_features(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "    # Normalize and combine embeddings\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    combined_embeddings = (image_features + text_features) / 2.0\n",
    "    combined_embeddings = combined_embeddings / combined_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Project embeddings to match Stable Diffusion's expected dimension\n",
    "    projected_embeddings = projection_layer(combined_embeddings)\n",
    "\n",
    "    # Reshape embeddings to match Stable Diffusion's expected shape\n",
    "    projected_embeddings = projected_embeddings.unsqueeze(1)  # Add seq_len dimension (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    # Create negative_prompt_embeds (typically all zeros or empty embeddings)\n",
    "    negative_prompt_embeds = torch.zeros_like(projected_embeddings)\n",
    "\n",
    "\n",
    "    # Generate the image using the Stable Diffusion pipeline\n",
    "    generated_images = stable_diffusion(\n",
    "          prompt_embeds=projected_embeddings,\n",
    "          negative_prompt_embeds=negative_prompt_embeds,  # Provide negative embeddings\n",
    "          image=input_image,  # Ensure this is a PIL.Image.Image object\n",
    "          strength=strength,  # Control the level of deviation from the input image\n",
    "          guidance_scale=guidance_scale,  # Control adherence to the prompt\n",
    "      )[\"images\"]\n",
    "\n",
    "    return generated_images[0]\n",
    "\n",
    "def calculate_clip_score(clip_model, clip_processor, image, text):\n",
    "    # Ensure the CLIP model is on the correct device\n",
    "    device = next(clip_model.parameters()).device\n",
    "\n",
    "    # Preprocess the image and text\n",
    "    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Ensure tensors match the model's data type\n",
    "    inputs = {\n",
    "        key: value.to(dtype=torch.long if key == \"input_ids\" else clip_model.dtype)\n",
    "        for key, value in inputs.items()\n",
    "    }\n",
    "\n",
    "    # Run the model and calculate the score\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "\n",
    "    return outputs.logits_per_image.item()\n",
    "\n",
    "\n",
    "def truncate_prompt_with_addition(base_prompt, addition, max_length=77):\n",
    "    \"\"\"\n",
    "    Truncate the base prompt to fit within the token limit while appending the addition.\n",
    "    Ensures the resulting prompt is exactly max_length tokens.\n",
    "    \"\"\"\n",
    "    from transformers import CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Tokenize base prompt and addition\n",
    "    base_tokens = tokenizer(base_prompt, truncation=False, return_tensors=\"pt\")\n",
    "    addition_tokens = tokenizer(addition, truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "    # Calculate available space for base prompt after accounting for the addition\n",
    "    total_tokens = len(base_tokens[\"input_ids\"][0]) + len(addition_tokens[\"input_ids\"][0]) - 1\n",
    "    if total_tokens > max_length:\n",
    "        max_base_length = max_length - (len(addition_tokens[\"input_ids\"][0]) - 1)\n",
    "        base_tokens[\"input_ids\"] = base_tokens[\"input_ids\"][:, :max_base_length]\n",
    "\n",
    "    # Combine truncated base prompt and addition tokens\n",
    "    combined_tokens = torch.cat((base_tokens[\"input_ids\"][0], addition_tokens[\"input_ids\"][0][1:]))\n",
    "    combined_tokens = combined_tokens[:max_length]  # Ensure final truncation to max_length\n",
    "\n",
    "    # Decode back to prompt text\n",
    "    truncated_prompt = tokenizer.decode(combined_tokens, skip_special_tokens=True)\n",
    "    print(f\"Updated Prompt: {truncated_prompt}\")\n",
    "    return truncated_prompt\n",
    "\n",
    "def calculate_ssim(image1, image2):\n",
    "    image1 = np.array(image1.convert(\"L\"))\n",
    "    image2 = np.array(image2.convert(\"L\"))\n",
    "    ssim, _ = compare_ssim(image1, image2, full=True)\n",
    "    return ssim\n",
    "\n",
    "def calculate_psnr(image1, image2):\n",
    "    image1 = np.array(image1)\n",
    "    image2 = np.array(image2)\n",
    "    return compare_psnr(image1, image2, data_range=255)\n",
    "\n",
    "def get_clip_embedding(clip_model, clip_processor, text_prompt, image_path=None, device=\"cuda\"):\n",
    "    # Compute CLIP embeddings for text and optional image\n",
    "    if image_path:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = clip_processor(text=[text_prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    else:\n",
    "        inputs = clip_processor(text=[text_prompt], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "\n",
    "    return outputs.text_embeds  # Use outputs.image_embeds for image embeddings if needed\n",
    "\n",
    "\n",
    "def project_clip_embedding(clip_embedding, target_dim=768, device=\"cuda\"):\n",
    "    # Ensure the embedding is compatible with Stable Diffusion\n",
    "    projection = torch.nn.Linear(clip_embedding.shape[-1], target_dim).to(device)\n",
    "    return projection(clip_embedding)\n",
    "\n",
    "\n",
    "def process_with_clip_conditioning(initial_prompt, image_path, clip_model, clip_processor, stable_diffusion,  device=\"cuda\", num_iterations=5\n",
    "):\n",
    "    enhanced_prompt = (\n",
    "        \"highly detailed police sketch, black and white pencil drawing, \"\n",
    "        \"professional forensic artist style, detailed shading, \"\n",
    "        f\"portrait of {initial_prompt}\"\n",
    "    )\n",
    "    lpips_model = LPIPS(net=\"alex\").to(device)\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    ssim_scores, psnr_scores, lpips_scores, clip_scores, generated_images = [], [], [], [], []\n",
    "    latents = None  # Initialize latents\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "\n",
    "        # Update the prompt for the next iteration\n",
    "        if i == 0:\n",
    "            addition = \", make the hair curlier\"\n",
    "        elif i == 1:\n",
    "            addition = \", make eyes larger\"\n",
    "        elif i == 2:\n",
    "            addition = \", make the eyebrows thinner\"\n",
    "        else:\n",
    "            addition = \", make the jawline more square\"\n",
    "\n",
    "        enhanced_prompt = truncate_prompt_with_addition(enhanced_prompt, addition, max_length=77)\n",
    "\n",
    "\n",
    "        if i == 0:\n",
    "          generated_image = generate_image_with_sketch_and_embeddings(\n",
    "            image_path, enhanced_prompt, clip_model, clip_processor, stable_diffusion, strength=0.4, guidance_scale=7.5\n",
    "          )\n",
    "          print(f\"\\nIteration {i + 1}: Generating image with updated prompt...{enhanced_prompt}\")\n",
    "        else:\n",
    "          generated_image = generate_image_with_sketch_and_embeddings(\n",
    "            image_path, addition, clip_model, clip_processor, stable_diffusion, strength=0.3, guidance_scale=7.5\n",
    "          )\n",
    "\n",
    "          print(f\"\\nIteration {i + 1}: Generating image with updated prompt...{addition}\")\n",
    "\n",
    "        generated_images.append(generated_image)\n",
    "\n",
    "\n",
    "        display(generated_image)\n",
    "        generated_image.save(f\"output_image_iteration_{i}.png\")\n",
    "        image_path = f\"output_image_iteration_{i}.png\"\n",
    "\n",
    "        resized_input_image = input_image.resize(generated_image.size)\n",
    "\n",
    "        ssim = calculate_ssim(resized_input_image, generated_image)\n",
    "        psnr = calculate_psnr(resized_input_image, generated_image)\n",
    "        lpips_value = lpips_model(\n",
    "            torch.tensor(np.array(resized_input_image)).permute(2, 0, 1).unsqueeze(0).to(device, dtype=torch.float32),\n",
    "            torch.tensor(np.array(generated_image)).permute(2, 0, 1).unsqueeze(0).to(device, dtype=torch.float32),\n",
    "        ).item()\n",
    "        clip_score = calculate_clip_score(clip_model, clip_processor, generated_image, enhanced_prompt)\n",
    "\n",
    "\n",
    "        clip_scores.append(clip_score)\n",
    "        ssim_scores.append(ssim)\n",
    "        psnr_scores.append(psnr)\n",
    "        lpips_scores.append(lpips_value)\n",
    "\n",
    "        print(f\"SSIM: {ssim}, PSNR: {psnr}, LPIPS: {lpips_value}, CLIP: {clip_score}\")\n",
    "\n",
    "    return ssim_scores, psnr_scores, lpips_scores, clip_scores, generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"The person is described as male around 30-35 years old with an oval face, defined cheekbones, medium-length straight hair, and light skin tone.\"\n",
    "image_path = \"/content/00002.jpg\"\n",
    "\n",
    "checkpoint_path = \"/content/drive/My Drive/path_to_your_file/clip_checkpoint_epoch_20.pt\"\n",
    "clip_model, clip_processor = setup_clip_model(checkpoint_path, device)\n",
    "stable_diffusion = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate images with CLIP embedding conditioning\n",
    "ssim_scores, psnr_scores, lpips_scores, clip_scores, generated_images = process_with_clip_conditioning(\n",
    "    text_prompt, image_path, clip_model, clip_processor, stable_diffusion\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nFinal Metrics Across Iterations:\")\n",
    "print(f\"SSIM Scores: {ssim_scores}\")\n",
    "print(f\"PSNR Scores: {psnr_scores}\")\n",
    "print(f\"LPIPS Scores: {lpips_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for i, img in enumerate(generated_images):\n",
    "    plt.subplot(1, len(generated_images), i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Iteration {i + 1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Metrics Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = list(range(1, len(ssim_scores) + 1))\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# SSIM Plot\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(iterations, ssim_scores, label=\"SSIM\", marker=\"o\", color=\"blue\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"SSIM\")\n",
    "plt.title(\"SSIM Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# PSNR Plot\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(iterations, psnr_scores, label=\"PSNR\", marker=\"o\", color=\"green\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.title(\"PSNR Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(iterations, clip_scores, label=\"CLIP Score\", marker='o', color='purple')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"CLIP Score\")\n",
    "plt.title(\"CLIP Score Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# LPIPS Plot\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(iterations, lpips_scores, label=\"LPIPS\", marker=\"o\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"LPIPS\")\n",
    "plt.title(\"LPIPS Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
