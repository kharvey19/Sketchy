{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from lpips import LPIPS  # Requires installation of lpips\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model for similarity scoring\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load LPIPS model for perceptual similarity\n",
    "lpips_model = LPIPS(net=\"alex\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_prompt(prompt, max_length=77):\n",
    "    \"\"\"\n",
    "    Truncate the prompt to fit within the token limit for the CLIP model, considering tokenized length.\n",
    "    \"\"\"\n",
    "    from transformers import CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokens = tokenizer(prompt, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    # Decode back the truncated prompt\n",
    "    truncated_prompt = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    print(f\"Prompt truncated to: {truncated_prompt}\")\n",
    "    return truncated_prompt\n",
    "\n",
    "# Function to calculate SSIM\n",
    "def calculate_ssim(image1, image2):\n",
    "    image1 = np.array(image1.convert(\"L\"))\n",
    "    image2 = np.array(image2.convert(\"L\"))\n",
    "    ssim, _ = compare_ssim(image1, image2, full=True)\n",
    "    return ssim\n",
    "\n",
    "# Function to calculate PSNR\n",
    "def calculate_psnr(image1, image2):\n",
    "    image1 = np.array(image1)\n",
    "    image2 = np.array(image2)\n",
    "    return compare_psnr(image1, image2, data_range=255)\n",
    "\n",
    "# Function to calculate CLIP similarity\n",
    "def calculate_clip_score(image, text):\n",
    "    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    outputs = clip_model(**inputs)\n",
    "    return outputs.logits_per_image.item()\n",
    "\n",
    "def preprocess_latents(pipeline, image, generator):\n",
    "    \"\"\"\n",
    "    Preprocess input image into latents for use in Img2Img pipeline.\n",
    "    \"\"\"\n",
    "    device = pipeline.device\n",
    "    image = image.to(device).to(dtype=torch.float16)  # Ensure the input image matches model precision\n",
    "    latent_image = pipeline.vae.encode(image.to(device)).latent_dist.sample(generator)\n",
    "    latent_image = latent_image * 0.18215  # Scale factor used in Stable Diffusion pipelines\n",
    "    return latent_image\n",
    "\n",
    "def setup_img2img_pipeline():\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    return pipe\n",
    "\n",
    "def truncate_prompt_with_addition(base_prompt, addition, max_length=77):\n",
    "    \"\"\"\n",
    "    Truncate the base prompt to fit within the token limit while appending the addition.\n",
    "    Ensures the resulting prompt is exactly max_length tokens.\n",
    "    \"\"\"\n",
    "    from transformers import CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Tokenize base prompt and addition\n",
    "    base_tokens = tokenizer(base_prompt, truncation=False, return_tensors=\"pt\")\n",
    "    addition_tokens = tokenizer(addition, truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "    # Calculate available space for base prompt after accounting for the addition\n",
    "    total_tokens = len(base_tokens[\"input_ids\"][0]) + len(addition_tokens[\"input_ids\"][0]) - 1\n",
    "    if total_tokens > max_length:\n",
    "        max_base_length = max_length - (len(addition_tokens[\"input_ids\"][0]) - 1)\n",
    "        base_tokens[\"input_ids\"] = base_tokens[\"input_ids\"][:, :max_base_length]\n",
    "\n",
    "    # Combine truncated base prompt and addition tokens\n",
    "    combined_tokens = torch.cat((base_tokens[\"input_ids\"][0], addition_tokens[\"input_ids\"][0][1:]))\n",
    "    combined_tokens = combined_tokens[:max_length]  # Ensure final truncation to max_length\n",
    "\n",
    "    # Decode back to prompt text\n",
    "    truncated_prompt = tokenizer.decode(combined_tokens, skip_special_tokens=True)\n",
    "    print(f\"Updated Prompt: {truncated_prompt}\")\n",
    "    return truncated_prompt\n",
    "\n",
    "def process_enhanced_prompt(input_image_path, input_prompt, num_iterations=5, strength=0.3, guidance_scale=7.5):\n",
    "    pipeline = setup_img2img_pipeline()\n",
    "\n",
    "    # Define the initial enhanced prompt\n",
    "    prompt = (\n",
    "        \"highly detailed police sketch, black and white pencil drawing, \"\n",
    "        \"professional forensic artist style \" + input_prompt\n",
    "    )\n",
    "\n",
    "    # Load the input image\n",
    "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "    input_image_tensor = torch.tensor(np.array(input_image)).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "    input_image_tensor = input_image_tensor.to(pipeline.device).to(dtype=torch.float16)\n",
    "\n",
    "    # Preprocess input image into latents\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    latents = preprocess_latents(pipeline, input_image_tensor, generator)\n",
    "\n",
    "    # Lists to store metrics\n",
    "    ssim_scores, psnr_scores, lpips_scores, clip_scores, generated_images = [], [], [], [], []\n",
    "\n",
    "    generated_image = None\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "\n",
    "        print(f\"Iteration {i + 1}: Generating image with prompt: {prompt}\")\n",
    "\n",
    "        if i == 0:\n",
    "            addition = \"with a thinner nose\"\n",
    "        elif i == 1:\n",
    "            addition = \"with thinner eyes\"\n",
    "        elif i == 2:\n",
    "            addition = \"with a more square jawline\"\n",
    "        elif i == 3:\n",
    "            addition = \"with a wider mouth and thinner lips\"\n",
    "        else:\n",
    "            addition = \"with darker eyebrows\"\n",
    "\n",
    "        if i != 0:\n",
    "          # Update latents after each iteration\n",
    "          latents = preprocess_latents(pipeline, torch.tensor(np.array(generated_image)).permute(2, 0, 1).unsqueeze(0).float() / 255.0, generator)\n",
    "\n",
    "        # Update the prompt with the addition, ensuring it remains 77 tokens\n",
    "        prompt = truncate_prompt_with_addition(prompt, addition, max_length=77)\n",
    "\n",
    "        # Generate an image using Img2Img with latents\n",
    "        output = pipeline(prompt=prompt, negative_prompt=\"no changes to other features\",image=input_image, latents=latents, strength=strength, guidance_scale=guidance_scale)\n",
    "\n",
    "        generated_image = output.images[0]\n",
    "\n",
    "        # Resize the input image to match the generated image dimensions\n",
    "        resized_input_image = input_image.resize(generated_image.size)\n",
    "\n",
    "        generated_images.append(generated_image)\n",
    "\n",
    "        # Display the generated image\n",
    "        display(generated_image)\n",
    "\n",
    "        # Calculate metrics\n",
    "        ssim = calculate_ssim(resized_input_image, generated_image)\n",
    "        ssim_scores.append(ssim)\n",
    "        print(f\"SSIM with input image at iteration {i + 1}: {ssim}\")\n",
    "\n",
    "        psnr = calculate_psnr(resized_input_image, generated_image)\n",
    "        psnr_scores.append(psnr)\n",
    "        print(f\"PSNR with input image at iteration {i + 1}: {psnr}\")\n",
    "\n",
    "        lpips_value = lpips_model(\n",
    "            torch.tensor(np.array(resized_input_image)).permute(2, 0, 1).unsqueeze(0).float().to(\"cuda\"),\n",
    "            torch.tensor(np.array(generated_image)).permute(2, 0, 1).unsqueeze(0).float().to(\"cuda\")\n",
    "        ).item()\n",
    "        lpips_scores.append(lpips_value)\n",
    "        print(f\"LPIPS with input image at iteration {i + 1}: {lpips_value}\")\n",
    "\n",
    "        clip_score = calculate_clip_score(generated_image, prompt)\n",
    "        clip_scores.append(clip_score)\n",
    "        print(f\"CLIP score for iteration {i + 1}: {clip_score}\")\n",
    "\n",
    "    return ssim_scores, psnr_scores, lpips_scores, clip_scores, generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_image_path = \"../input_images/00002.jpg\"\n",
    "prompt = \"a male around 30-35 years old The hairstyle is medium-length, straight hair parted slightly off-center and almond-shaped eyes with a neutral expression mouth\"\n",
    "\n",
    "# Process the enhanced prompt and calculate metrics\n",
    "ssim_scores, psnr_scores, lpips_scores, clip_scores, generated_images = process_enhanced_prompt(input_image_path, prompt)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFinal Metrics Across Iterations:\")\n",
    "print(f\"SSIM Scores: {ssim_scores}\")\n",
    "print(f\"PSNR Scores: {psnr_scores}\")\n",
    "print(f\"LPIPS Scores: {lpips_scores}\")\n",
    "print(f\"CLIP Scores: {clip_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for i, img in enumerate(generated_images):\n",
    "    plt.subplot(1, len(generated_images), i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Iteration {i + 1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Metrics Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = list(range(1, len(ssim_scores) + 1))\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# SSIM Plot\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(iterations, ssim_scores, label=\"SSIM\", marker='o', color='blue')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"SSIM\")\n",
    "plt.title(\"SSIM Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# PSNR Plot\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(iterations, psnr_scores, label=\"PSNR\", marker='o', color='green')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.title(\"PSNR Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# CLIP Score Plot\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(iterations, clip_scores, label=\"CLIP Score\", marker='o', color='purple')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"CLIP Score\")\n",
    "plt.title(\"CLIP Score Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "# LPIPS Plot\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(iterations, lpips_scores, label=\"LPIPS\", marker='o', color='orange')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"LPIPS\")\n",
    "plt.title(\"LPIPS Over Iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
