{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katherine-harvey/miniconda3/envs/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load Stable Diffusion img2img pipeline\n",
    "stable_diffusion = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = nn.Linear(512, 768).to(\"cuda\")\n",
    "\n",
    "def generate_image_with_sketch_and_embeddings(\n",
    "    input_image_path, prompt, strength=0.3, guidance_scale=7.5\n",
    "):\n",
    "    # Load and preprocess the input sketch\n",
    "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "        \n",
    "    # Preprocess the image and prompt for CLIP\n",
    "    inputs = clip_processor(text=[prompt], images=input_image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate CLIP embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"]).to(\"cuda\")\n",
    "        text_features = clip_model.get_text_features(input_ids=inputs[\"input_ids\"]).to(\"cuda\")\n",
    "\n",
    "    # Normalize and combine embeddings\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    combined_embeddings = (image_features + text_features) / 2.0\n",
    "\n",
    "    # Project embeddings to match Stable Diffusion's expected dimension\n",
    "    projected_embeddings = projection_layer(combined_embeddings)\n",
    "\n",
    "    # Reshape embeddings to match Stable Diffusion's expected shape\n",
    "    projected_embeddings = projected_embeddings.unsqueeze(1)  # Add seq_len dimension (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    # Create negative_prompt_embeds (typically all zeros or empty embeddings)\n",
    "    negative_prompt_embeds = torch.zeros_like(projected_embeddings)\n",
    "\n",
    "    # Generate the image using the img2img pipeline\n",
    "    generated_images = stable_diffusion(\n",
    "        prompt_embeds=projected_embeddings,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,  # Provide negative embeddings\n",
    "        image=input_image,  # Ensure this is a PIL.Image.Image object\n",
    "        strength=strength,  # Control the level of deviation from the input image\n",
    "        guidance_scale=guidance_scale,  # Control adherence to the prompt\n",
    "    )[\"images\"]\n",
    "\n",
    "    return generated_images[0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = \"../input_images/00002.jpg\"\n",
    "prompt = \"The person is described as male around 30-35 years old with an oval face, defined cheekbones, medium-length straight hair, and light skin tone.\"\n",
    "\n",
    "# Generate the image\n",
    "generated_image = generate_image_with_sketch_and_embeddings(\n",
    "    input_image_path, prompt, strength=0.3, guidance_scale=7.5\n",
    ")\n",
    "\n",
    "# Save the generated imagea\n",
    "generated_image.save(\"output_with_sketch_and_embeddings.png\")\n",
    "display(generated_image)\n",
    "print(\"Image saved as 'output_with_sketch_and_embeddings.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
