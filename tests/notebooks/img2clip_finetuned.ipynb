{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear, r, lora_alpha, lora_dropout):\n",
    "        super().__init__()\n",
    "        self.original_linear = original_linear\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(original_linear.in_features, r))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(r, original_linear.out_features))\n",
    "        self.scaling = lora_alpha / r\n",
    "\n",
    "        # Initialize parameters\n",
    "        nn.init.normal_(self.lora_A, std=0.02)\n",
    "        nn.init.normal_(self.lora_B, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.original_linear(x)\n",
    "        if self.r > 0:\n",
    "            lora_result = (self.lora_dropout(x) @ self.lora_A @ self.lora_B) * self.scaling\n",
    "            return result + lora_result\n",
    "        return result\n",
    "\n",
    "\n",
    "def apply_lora_to_model(model, r=16, lora_alpha=32, lora_dropout=0.1):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(substr in name for substr in ['q_proj', 'v_proj', 'k_proj']):\n",
    "            parent = model\n",
    "            for name_part in name.split('.')[:-1]:\n",
    "                parent = getattr(parent, name_part)\n",
    "            layer_name = name.split('.')[-1]\n",
    "            setattr(parent, layer_name, LoRALinear(module, r, lora_alpha, lora_dropout))\n",
    "\n",
    "\n",
    "def setup_clip_model(checkpoint_path, device):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    apply_lora_to_model(model)  # Apply LoRA modifications\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "    model = model.to(device).eval()\n",
    "    return model, processor\n",
    "\n",
    "projection_layer = nn.Linear(512, 768).to(\"cuda\")\n",
    "\n",
    "def generate_image_with_sketch_and_embeddings(\n",
    "    input_image_path, prompt, clip_model, clip_processor, stable_diffusion, device=\"cuda\", strength=0.3, guidance_scale=7.5\n",
    "):\n",
    "    # Load and preprocess the input sketch\n",
    "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "    # Preprocess the image and prompt for CLIP\n",
    "    inputs = clip_processor(text=[prompt], images=input_image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Generate CLIP embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        text_features = clip_model.get_text_features(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "    # Normalize and combine embeddings\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    combined_embeddings = (image_features + text_features) / 2.0\n",
    "    combined_embeddings = combined_embeddings / combined_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Project embeddings to match Stable Diffusion's expected dimension\n",
    "    projected_embeddings = projection_layer(combined_embeddings)\n",
    "\n",
    "    # Reshape embeddings to match Stable Diffusion's expected shape\n",
    "    projected_embeddings = projected_embeddings.unsqueeze(1)  # Add seq_len dimension (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    # Create negative_prompt_embeds (typically all zeros or empty embeddings)\n",
    "    negative_prompt_embeds = torch.zeros_like(projected_embeddings)\n",
    "    \n",
    "    # Generate the image using the Stable Diffusion pipeline\n",
    "    generated_images = stable_diffusion(\n",
    "          prompt_embeds=projected_embeddings,\n",
    "          negative_prompt_embeds=negative_prompt_embeds,  # Provide negative embeddings\n",
    "          image=input_image,  # Ensure this is a PIL.Image.Image object\n",
    "          strength=strength,  # Control the level of deviation from the input image\n",
    "          guidance_scale=guidance_scale,  # Control adherence to the prompt\n",
    "      )[\"images\"]\n",
    "\n",
    "\n",
    "    return generated_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "checkpoint_path = \"/content/drive/My Drive/clip_checkpoint_epoch_20.pt\"\n",
    "clip_model, clip_processor = setup_clip_model(checkpoint_path, device)\n",
    "stable_diffusion = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Example usage\n",
    "text_prompt = \"The person is described as male around 30-35 years old with an oval face, defined cheekbones, medium-length straight hair, and light skin tone.\"\n",
    "image_path = \"/content/00002.jpg\"\n",
    "generated_image = generate_image_with_sketch_and_embeddings(\n",
    "    image_path, text_prompt, clip_model, clip_processor, stable_diffusion, device\n",
    ")\n",
    "\n",
    "generated_image.save(\"output_with_sketch_and_embeddings.png\")\n",
    "display(generated_image)\n",
    "print(\"Image saved as 'output_with_sketch_and_embeddings.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
